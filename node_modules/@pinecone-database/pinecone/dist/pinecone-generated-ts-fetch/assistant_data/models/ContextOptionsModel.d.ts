/**
 * Pinecone Assistant Data Plane API
 * Pinecone Assistant Engine is a context engine to store and retrieve relevant knowledge from millions of documents at scale. This API supports interactions with assistants.
 *
 * The version of the OpenAPI document: 2025-10
 * Contact: support@pinecone.io
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */
/**
 * Controls the context snippets sent to the LLM.
 * @export
 * @interface ContextOptionsModel
 */
export interface ContextOptionsModel {
    /**
     * The maximum number of context snippets to use. Default is 16. Maximum is 64.
     * @type {number}
     * @memberof ContextOptionsModel
     */
    topK?: number;
    /**
     * The maximum context snippet size. Default is 2048 tokens. Minimum is 512 tokens. Maximum is 8192 tokens.
     * @type {number}
     * @memberof ContextOptionsModel
     */
    snippetSize?: number;
    /**
     * Whether or not to send image-related context snippets to the LLM. If `false`, only text context snippets are sent.
     * @type {boolean}
     * @memberof ContextOptionsModel
     */
    multimodal?: boolean;
    /**
     * If image-related context snippets are sent to the LLM, this field determines whether or not they should include base64 image data. If `false`, only the image caption is sent. Only available when `multimodal=true`.
     * @type {boolean}
     * @memberof ContextOptionsModel
     */
    includeBinaryContent?: boolean;
}
/**
 * Check if a given object implements the ContextOptionsModel interface.
 */
export declare function instanceOfContextOptionsModel(value: object): boolean;
export declare function ContextOptionsModelFromJSON(json: any): ContextOptionsModel;
export declare function ContextOptionsModelFromJSONTyped(json: any, ignoreDiscriminator: boolean): ContextOptionsModel;
export declare function ContextOptionsModelToJSON(value?: ContextOptionsModel | null): any;
